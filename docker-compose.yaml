# sudo docker compose -f docker-compose.yaml up 

services:
# BEGIN kafka setup
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks: 
      tap: 
        ipv4_address: 10.0.100.31

  kafka-broker:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka-broker
    ports:
      - 9092:9092
      - '29092:29092'
    depends_on:
      - zookeeper
    environment:  
      KAFKA_BROKER_ID: 1
      CONNECT_BOOTSTRAP_SERVERS: kafka-broker:9092
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      #  Kafka broker listens on localhost:9092 for access from your host machine and on 10.0.100.32:29092 for access from other containers
      #  listener for internal use was set to explicit ip to permit local spark driver script execution. netsh interface ip add address "Loopback" 10.0.100.32,
      #  because driver outside docker will connect to kafka, and then pass url to internal docker spark worker
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://10.0.100.32:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_MS: 3000000 # 50 min retention
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 3000000
    networks: 
      tap: 
        ipv4_address: 10.0.100.32

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:master
    ports:
      - 8080:8080
    depends_on:
      - zookeeper
      - kafka-broker
    environment:
      SERVER_PORT: 8080
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks: 
      tap: 
        ipv4_address: 10.0.100.33
# END kafka setup

  logstash:
    build:
      context: .
      dockerfile: logstash/Dockerfile
    container_name: logstash
    # depends_on: 
    #   - kafka-broker
    networks: 
      tap: 
        ipv4_address: 10.0.100.30
    volumes:
      - /home/ignaziogulino/ProgettoTAP/logstash/pipeline:/usr/share/logstash/pipeline/

# BEGIN spark cluster setup
  spark-master: 
    build:
      context: ./spark-local
      dockerfile: setup-master/Dockerfile 
    container_name: spark-master
    ports:
      - "8081:8081" #web ui
      - "7077:7077" # master port
      - "4040:4040"
    environment:
      SPARK_MASTER_IP: 10.0.100.34
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_PORT_UI: 8081
    networks:
      tap:
        ipv4_address: 10.0.100.34
    volumes:
      - /home/ignaziogulino/ProgettoTAP/spark-local/code:/opt/tap-project/code/
      - nlp-shared:/root/cache_pretrained/
        
  spark-worker: 
    build:
      context: ./spark-local
      dockerfile: setup-worker/Dockerfile 
    container_name: spark-worker
    depends_on:
      - spark-master
    ports:
      - "8082:8082" # web-ui
      - "7078:7078" # worker port
    environment:
      SPARK_MASTER_IP: 10.0.100.34
      SPARK_MASTER_PORT: 7077
      SPARK_WORKER_PORT: 7078
      SPARK_WORKER_PORT_UI: 8082
    networks:
      tap:
        ipv4_address: 10.0.100.35
    volumes:
      - nlp-shared:/root/cache_pretrained/

  spark-driver: 
    build:
      context: ./spark-local
      dockerfile: setup-driver/Dockerfile 
    container_name: spark-driver
    networks:
      tap:
        ipv4_address: 10.0.100.38
    volumes:
    - /home/ignaziogulino/ProgettoTAP/spark-local/code:/opt/tap-project/code/
    - nlp-shared:/root/cache_pretrained/
  # END spark cluster setup

  elastic-search: 
    container_name: elastisearch
    image: docker.elastic.co/elasticsearch/elasticsearch:7.11.0
    environment:
      #disable security eg auth
      - xpack.security.enabled=false
      # binds to loopback http for internal communication: must be used when ES is in the same producer machine
      - discovery.type=single-node
    networks:
      tap:
        ipv4_address: 10.0.100.36
    ports:
      - 9200:9200

  kibana:
    container_name: kibana
    image: docker.elastic.co/kibana/kibana:7.11.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elastisearch:9200
    networks:
      tap:
        ipv4_address: 10.0.100.37
    depends_on:
      - elasticsearch
    ports:
      - 5601:5601

volumes:
# used to share pre-trained model
  nlp-shared:

networks:
  tap:
    external: true
